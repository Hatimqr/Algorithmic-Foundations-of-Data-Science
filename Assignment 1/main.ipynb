{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0679dc1",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "\n",
    "Assumptions:\n",
    "1. If there is a line (x, y), there will not be another line with (y, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91583038",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "\n",
    "Map function:\n",
    "$$\n",
    "map : (null, (x,y)) \\rightarrow [(x, 1), (y, 1)]\n",
    "$$\n",
    "Reduce function:\n",
    "$$\n",
    "reduce : (x, [1, 1, ...]) \\rightarrow (x, sum([1, 1, ...]))\n",
    "$$\n",
    "\n",
    "The file would have each line: $x, \\text{count\\_of\\_friends}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d047fd22",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "### Phase 1\n",
    "\n",
    "Map function:\n",
    "$$\n",
    "map : (null, (x,y)) \\rightarrow [(x, y), (y, x)]\n",
    "$$\n",
    "\n",
    "\n",
    "Reduce function (identity):\n",
    "$$\n",
    "reduce : (x, [y_1, y_2, ...]) \\rightarrow (x, [y_1, y_2, ...])\n",
    "$$\n",
    "\n",
    "\n",
    "### Intemediary Step (optional)\n",
    "Filter out all pairs where the length of the friend list < k\n",
    "\n",
    "\n",
    "### Phase 2\n",
    "\n",
    "Map Function:\n",
    "$$\n",
    "map : (x_i, [y_1, y_2, ...]) \\rightarrow ((y_i, y_j), 1) \\forall i,j, i \\neq j\n",
    "$$\n",
    "\n",
    "Reduce Function:\n",
    "$$\n",
    "reduce : ((y_i, y_j), [1,1,...]) \\rightarrow ((y_i, y_j), sum([1,1,...]))\n",
    "$$\n",
    "Write the $(y_i, y_j)$ pairs that have key $\\geq 7$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f52cd2",
   "metadata": {},
   "source": [
    "## Part C\n",
    "\n",
    "Map function:\n",
    "$$\n",
    "map: (x,y) \\rightarrow ((x, 1), (y, 1))\n",
    "$$\n",
    "Reduce function:\n",
    "$$\n",
    "reduce: (x, [1, 1, ...]) \\rightarrow \n",
    "\\begin{cases}\n",
    "    (x, \\text{null}) & \\text{with probability } 0.01 \\\\\n",
    "    \\text{nothing} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beed102",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "Assumptions:\n",
    "1. reducer size is the input size of that particular reducer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9b225e",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "Suppose not, that there is a reducer, $R$, that solves our problem and has a required input size of $n+k$ for some $k>0$.\n",
    "\n",
    "By definition, this means the reducer $R$ is an algorithm that requires an input sequence of length $n+k$ to operate correctly.\n",
    "\n",
    "But our problem's input is only of size $n$, the sequence $(a_1,..., a_n)$. When we provide this input to reducer $R$, it cannot run because its definition expects elements at indices larger than $n$, which simply don't exist in the input we have.\n",
    "\n",
    "This is a contradiction, because $R$ cannot solve the problem if it can't even run on the problem's input. Thus, the reducer size can be at most $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e4a89",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "### Idea\n",
    "First we make a key observation. To compute $m_i$, we can do $m_i = min/{m_{i-1}, a_i/}$ instead of comparing all $a_k$ where $k < i$.\n",
    "\n",
    "We can do 2 logical steps as follows:\n",
    "1. Split the input sequence into $\\frac{n}{sqrt{n}}$ equal and ordered chunks and compute the min for the largest index in the chunk. \n",
    "2. now we will have $\\sqrt{n}$ pairs of local minima and we perform the naive version of the algorithm but instead of using all $a_i$ we use only the $a_i$'s that are greater than the largest $m_i$ before the target $m_i$. Additionally we must use all the previous $m_i$'s as well (but we could compute a running minima of all intermediate m_i's given a third pass). This will make the reducer size in the second pass as most $(\\sqrt{n}-1)+(\\sqrt{n}-1)$ (the second to last element) which is still $O(\\sqrt{n})$\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "#### Pass 1 - chunk mins\n",
    "\n",
    "Break array $A$ into $\\sqrt{n}$ equal and ordered chunks $C_1,C_2,…,C_{\\sqrt{n}}$. For the mapping function we need to be able to map $i$ to a chunk id $c_i$. We do this as follows:\n",
    "\n",
    "$$c_i=\\lceil \\frac{i}{\\sqrt{n}} \\rceil$$\n",
    "\n",
    "\n",
    "Mapping function:\n",
    "$$\n",
    "map : (i, a_i) \\mapsto (c_i, a_i)\n",
    "$$\n",
    "\n",
    "Reduce function:\n",
    "$$\n",
    "reduce: (c_i, [a_i, ... , a_{i+\\sqrt{n}-1}]) \\mapsto (c_i, M_i)\n",
    "$$\n",
    "where $M_i = \\min\\{[a_i, ... , a_{i+\\sqrt{n}-1}]\\}$\n",
    "\n",
    "\n",
    "\n",
    "#### Pass 2 - Assemble\n",
    "\n",
    "We now have inputs:\n",
    "1. $(c_j, M_j)$\n",
    "2. $(i, a_i)$\n",
    "\n",
    "Mapping Function:\n",
    "$$\n",
    "map: (i, a_i) \\mapsto (c_i, ('val', i, a_i)) \\newline\n",
    "map: (c_j, M_j) \\mapsto (c_{k}, ('min', M_j)) \\space \\forall c_k>c_j\n",
    "$$\n",
    "\n",
    "After the shuffle phase:\n",
    " $$(c_i, [('min', M_1), ... , ('min', M_{i-\\sqrt{n}})], ('val', i, a_i), ... , ('val', i+\\sqrt{n}, a_{i+\\sqrt{n}}))$$\n",
    "\n",
    "Reduce Function:\n",
    "\n",
    "Each reducer is responsible for a single chunk $c_i$\n",
    "1. Calculate the running minimum from all minimum chunks:\n",
    "$$\n",
    "P = \\min\\{M_j | ('min', M_j) \\in list\\}\n",
    "$$\n",
    "\n",
    "2. Initialize local_min as P (if no 'min' tuples like in the first chunk, initialise to infinity)\n",
    "3. Sort the value tuples $('val', i, a_i)$ by $i$\n",
    "4. Iterate through the sorted value tuples\n",
    "    - for each $('val', i, a_i)$:\n",
    "        - update $local_min = min(local_min, a_i)$\n",
    "        - emit the final pair $(i, local_min)$\n",
    "\n",
    "We then combine the lists from all reducers and return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becb3397",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12216f60",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "The goal is to broadcast $x_0, ... x_{k+1}$ (k+2) numbers to t machines. \n",
    "\n",
    "The subsampling happens on the master so there is no communication cost involved. We simply pick each number with probability $k/n$.\n",
    "The broadcast is the only phase we have a communication cost.\n",
    "\n",
    "This means we are sending a total of $(k+2)\\times t = kt \\times 2t$ numbers in total. This is $O(kt)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958df34c",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "The goal here is to compute how many numbers fall into each of the $k$ buckets (exist on each machine).\n",
    "\n",
    "First we count the number of elements in each bucket on each of the $t$ local machines. We end up with a count for each of the $k$ buckets.\n",
    "\n",
    "Next, we need to transmit this count back to the master to sum up the total number of elements in each bucket. Here, we send $k$ (or 2k if we are sending pairs) numbers to the master from each of the $t$ machines. $G_i$ is then the sum of the count sent from each machine for each bucket.\n",
    "\n",
    "Thus, in total we are sending at most $2k \\times t$ numbers across the network which is $O(kt)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d021b",
   "metadata": {},
   "source": [
    "## Part C\n",
    "\n",
    "The goal here is to use the global counts that we have from the master to figure out which $G_i$ contains the median and then which element in $G_i$ is the median ($r$).\n",
    "\n",
    "We now have $(n_i, count_i)$ on the master. Wer can find $G_i$ and $r$ as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\text{1. Calculate the median's overall rank}\\\\\n",
    "&N = \\sum \\text{counts}\\\\\n",
    "&\\text{median\\_rank} = \\lceil N / 2 \\rceil\\\\\n",
    "\\\\\n",
    "&\\text{2. Find the bucket containing the median}\\\\\n",
    "&\\text{cumulative\\_count} = 0\\\\\n",
    "&\\text{for } j \\text{ from } 0 \\text{ to length(counts)} - 1\\text{:}\\\\\n",
    "&\\quad\\text{// Check if the median falls within the current bucket}\\\\\n",
    "&\\quad\\text{if } (\\text{cumulative\\_count} + \\text{counts}[j]) \\geq \\text{median\\_rank}\\text{:}\\\\\n",
    "&\\quad\\quad\\text{// 3. Calculate the rank 'r' within this bucket}\\\\\n",
    "&\\quad\\quad r = \\text{median\\_rank} - \\text{cumulative\\_count}\\\\\n",
    "&\\quad\\quad\\text{return } (j, r) \\text{ // Return the bucket index and the rank}\\\\\n",
    "\\\\\n",
    "&\\quad\\text{// If not found, add the current bucket's count to the total}\\\\\n",
    "&\\quad\\text{cumulative\\_count} = \\text{cumulative\\_count} + \\text{counts}[j]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "There is no communication cost since this is all done on the master."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb4e762",
   "metadata": {},
   "source": [
    "## Part D\n",
    "\n",
    "We now know $G_j$ and $r$ so we need to collect an ordered list of numbers in $G_j$ to find the median. \n",
    "\n",
    "The master instructs all $t$ machines to send it only the numbers they have that belong to bucket $G_j$. The total communication is the total number of elements in $G_j$, which is $n_j$.\n",
    "\n",
    "The network cost would be the expected size of $n_j$. Since we chose $k$ samples, the data would be divided into $k+1$ chunks of roughly equal size on average.\n",
    "\n",
    "Thus the network communication cost is approximately $\\frac{n}{k+1}$ which is $O(\\frac{n}{k})$\n",
    "\n",
    "Once the master has the list of numbers in $G_j$, we sort it and return the element at index $r$. This is the median.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2116a9",
   "metadata": {},
   "source": [
    "## Part E\n",
    "\n",
    "From the above parts we know communication cost is:\n",
    "$$\n",
    "Total Cost(k) \\approx O(kt) + O(kt) + O(n/k) = O(kt + n/k)\n",
    "$$\n",
    "\n",
    "To minimize network cost we need to find $k$ that minimizes $f(k)=kt + n/k$\n",
    "\n",
    "$$\n",
    "f'(k) = t-\\frac{n}{k^2} \\newline\n",
    "0 = t-\\frac{n}{k^2} \\newline\n",
    "k^2=\\frac{n}{t} \\newline\n",
    "k = \\sqrt{\\frac{n}{t}}\n",
    "$$\n",
    "\n",
    "Thus we choose $k = \\sqrt{\\frac{n}{t}}$ to minimize network cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dab23e",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db500f38",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d4d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "spark_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.4\n",
      "Spark UI available at: http://10.228.244.25:4040\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session for local machine\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Assignment1_Problem4\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to basically no verbose output\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark UI available at: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b966733",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasources = {\n",
    "    'links' : spark.read.csv('data/links.csv', header=True, inferSchema=True),\n",
    "    'movies' : spark.read.csv('data/movies.csv', header=True, inferSchema=True),\n",
    "    'ratings' : spark.read.csv('data/ratings.csv', header=True, inferSchema=True),\n",
    "    'tags' : spark.read.csv('data/tags.csv', header=True, inferSchema=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30d655e",
   "metadata": {},
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46f3c11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of ratings per movie:  10.369806663924312\n"
     ]
    }
   ],
   "source": [
    "df = datasources['ratings']\n",
    "\n",
    "avg_count = (\n",
    "    df.groupBy('movieID').agg(count('rating').alias('count')) #count ratings for each movie\n",
    "    .agg(avg('count')) # get the average counts\n",
    "    .collect()[0][0] # get the average\n",
    ")\n",
    "print('Average number of ratings per movie: ',avg_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cb6612",
   "metadata": {},
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfad1eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|              genres|avg_rating|\n",
      "+--------------------+----------+\n",
      "|Action|Crime|Dram...|       5.0|\n",
      "|Adventure|Drama|F...|       5.0|\n",
      "|Comedy|Drama|Fant...|       5.0|\n",
      "|Animation|Childre...|       5.0|\n",
      "|Action|Horror|Mys...|       5.0|\n",
      "|Adventure|Comedy|...|       5.0|\n",
      "|Drama|Fantasy|Mus...|       5.0|\n",
      "|Drama|Horror|Romance|       5.0|\n",
      "|Adventure|Romance...|       5.0|\n",
      "|Animation|Crime|D...|       5.0|\n",
      "|Fantasy|Mystery|W...|       5.0|\n",
      "|Action|Comedy|Dra...|       5.0|\n",
      "|Animation|Drama|F...|       5.0|\n",
      "|Comedy|Crime|Dram...|       5.0|\n",
      "|Animation|Drama|S...|       5.0|\n",
      "|Comedy|Horror|Mys...|       5.0|\n",
      "|Comedy|Crime|Fantasy|       5.0|\n",
      "|Comedy|Crime|Dram...|      4.75|\n",
      "|   Animation|Romance|      4.75|\n",
      "|Children|Drama|Ro...|      4.75|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies = datasources['movies']\n",
    "ratings = datasources['ratings']\n",
    "\n",
    "# join genre on movie id in ratings\n",
    "df = (\n",
    "    ratings.join(\n",
    "        movies.select('movieID', 'genres'),\n",
    "        on='movieID', \n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "\n",
    "genre_avg = (\n",
    "    df.groupBy('genres')\n",
    "    .agg(avg('rating').alias('avg_rating')) # get average rating for each genre\n",
    "    .sort('avg_rating', ascending=False) # sort by average rating\n",
    ")\n",
    "\n",
    "genre_avg.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2cd7bf",
   "metadata": {},
   "source": [
    "## Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d04f4ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+----+\n",
      "|               title|              genres|        avg_rating|rank|\n",
      "+--------------------+--------------------+------------------+----+\n",
      "|        Black Mirror|  (no genres listed)|               5.0|   1|\n",
      "|Death Note: Desu ...|  (no genres listed)|               5.0|   2|\n",
      "|The Adventures of...|  (no genres listed)|               5.0|   3|\n",
      "|    Knock Off (1998)|              Action|               5.0|   1|\n",
      "|Big Bird Cage, Th...|              Action|               4.5|   2|\n",
      "|Master of the Fly...|              Action|               4.5|   3|\n",
      "|Crippled Avengers...|    Action|Adventure|               5.0|   1|\n",
      "|Shogun Assassin (...|    Action|Adventure|               5.0|   2|\n",
      "|Touch of Zen, A (...|    Action|Adventure|               4.5|   3|\n",
      "|Dragon Ball Z: Br...|Action|Adventure|...|               4.0|   1|\n",
      "|Dragon Ball Z: Co...|Action|Adventure|...|               4.0|   2|\n",
      "|The Boy and the B...|Action|Adventure|...|               4.0|   3|\n",
      "|Dragon Ball: Myst...|Action|Adventure|...|               3.5|   1|\n",
      "|Dragon Ball: The ...|Action|Adventure|...|               3.5|   2|\n",
      "|Dragon Ball: The ...|Action|Adventure|...|               3.5|   3|\n",
      "|Alpha and Omega 3...|Action|Adventure|...|               4.5|   1|\n",
      "|Asterix and Cleop...|Action|Adventure|...|              4.25|   2|\n",
      "|     Zootopia (2016)|Action|Adventure|...|          3.890625|   3|\n",
      "|The Lego Movie (2...|Action|Adventure|...| 3.870967741935484|   1|\n",
      "|Twelve Tasks of A...|Action|Adventure|...|3.6666666666666665|   2|\n",
      "+--------------------+--------------------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies = datasources['movies']\n",
    "ratings = datasources['ratings']\n",
    "\n",
    "# we need ratings, movies, and genres\n",
    "df = (\n",
    "    ratings.join(\n",
    "        movies.select('movieID', 'genres', 'title'),\n",
    "        on='movieID',\n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "\n",
    "movie_avg = (\n",
    "    df.groupBy('movieID', 'title', 'genres')\n",
    "    .agg(avg('rating').alias('avg_rating')) # get average rating for each movie\n",
    "    # keep only the top 3 movies in each genre\n",
    "    .withColumn('rank', \n",
    "        row_number().over(Window.partitionBy('genres').orderBy(desc('avg_rating'))))\n",
    "    .filter('rank <= 3')\n",
    "    .sort('genres', 'rank')\n",
    "    .drop('movieID')\n",
    ")\n",
    "\n",
    "movie_avg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3291c620",
   "metadata": {},
   "source": [
    "## Part D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "851ece07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|userID|count_rated|\n",
      "+------+-----------+\n",
      "|   414|       2698|\n",
      "|   599|       2478|\n",
      "|   474|       2108|\n",
      "|   448|       1864|\n",
      "|   274|       1346|\n",
      "|   610|       1302|\n",
      "|    68|       1260|\n",
      "|   380|       1218|\n",
      "|   606|       1115|\n",
      "|   288|       1055|\n",
      "+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings = datasources['ratings']\n",
    "\n",
    "user_movies = (\n",
    "    ratings.select('userID', 'movieID')\n",
    "    .groupBy('userID')\n",
    "    .agg(count('movieID').alias('count_rated')) # count num ratings for each user\n",
    "    .sort('count_rated', ascending=False) # sort by num ratings\n",
    ")\n",
    "\n",
    "user_movies.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18993e6b",
   "metadata": {},
   "source": [
    "## Part E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87e2ebbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users: 610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 281:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------------------+\n",
      "|user_1|user_2|intersection_cardinality|\n",
      "+------+------+------------------------+\n",
      "|   414|   599|                    1338|\n",
      "|   599|   414|                    1338|\n",
      "|   414|   474|                    1077|\n",
      "|   474|   414|                    1077|\n",
      "|    68|   414|                     950|\n",
      "|   414|    68|                     950|\n",
      "|   414|   448|                     914|\n",
      "|   448|   414|                     914|\n",
      "|   274|   414|                     856|\n",
      "|   414|   274|                     856|\n",
      "+------+------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ratings = datasources['ratings']\n",
    "\n",
    "print('num_users:',ratings.select('userID').agg(countDistinct('userID')).collect()[0][0])\n",
    "\n",
    "# first we get a dataframe with (userID, list_of_movies_rated)\n",
    "user_movies = (\n",
    "    ratings.select('userID', 'movieID')\n",
    "    .groupBy('userID')\n",
    "    .agg(collect_list('movieID').alias('movies_rated'))\n",
    ")\n",
    "\n",
    "# now we need to check for every user pair what the intersection of their movies is\n",
    "# first do a cross join and remove rows where user1 == user2\n",
    "user_pairs = (\n",
    "    user_movies.alias('u1')\n",
    "    .crossJoin(user_movies.alias('u2'))\n",
    "    .filter('u1.userID != u2.userID')\n",
    "    .select(\n",
    "        col('u1.userID').alias('userID_u1'),\n",
    "        col('u1.movies_rated').alias('movies_rated_u1'),\n",
    "        col('u2.userID').alias('userID_u2'),\n",
    "        col('u2.movies_rated').alias('movies_rated_u2')\n",
    "    )\n",
    ")\n",
    "\n",
    "# now we need to create another column with the intersection between movies_rated_u1 and movies_rated_u2\n",
    "# also get the cardinality of the intersection\n",
    "user_pairs = (\n",
    "    user_pairs\n",
    "    .withColumn(\n",
    "        'movies_rated_intersection',\n",
    "        array_intersect('movies_rated_u1', 'movies_rated_u2')\n",
    "    )\n",
    "    .withColumn(\n",
    "        'intersection_cardinality',\n",
    "        size('movies_rated_intersection')\n",
    "    )\n",
    ")\n",
    "\n",
    "# rename cols and drop unnessary columns. sort by intersection_cardinality\n",
    "user_pairs = (\n",
    "    user_pairs\n",
    "    .withColumnRenamed('userID_u1', 'user_1')\n",
    "    .withColumnRenamed('userID_u2', 'user_2')\n",
    "    .drop('movies_rated_u1', 'movies_rated_u2', 'movies_rated_intersection')\n",
    "    .sort('intersection_cardinality', ascending=False)\n",
    ")\n",
    "\n",
    "\n",
    "user_pairs.show(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
